---
student_name: [Katriina Parkkinen]
topic: [How does the open weight model from gpt-oss-safeguard and ROOST align 
with digital commons and can online safety be community-led?]
---

# Introduction

On October 29th 2025, OpenAI and ROOST published the open source weights of gpt-oss-safeguard, a bring-your-own-policy model that allows developers to fine-tune safety classification tasks with the aim of making moderating online spaces easier. The model uses chain-of-thought reasoning, explaining the decision it takes to create transparency for the organizations using it. 

The model is available under the Apache 2.0 license, allowing it to be used, modified, and deployed freely, and it can be downloaded from Hugging Face in 20b or 120b versions. 

## How it works

The companies say that developers can draw policy lines that best fit their use case, the model explains how the content is classified under the policy, and developers can then decide if they want to use the conclusion by the model in their safety pipelines. It differs from traditional safety classifiers by passing in any policy

# Online safety and digital commons

## Elinor Ostromâ€™s 8 principles for governing the commons

## Improvements and recommendations

# Conclusion

# References
Hugging Face: https://huggingface.co/collections/openai/gpt-oss-safeguard  
OpenAI Product Release: https://openai.com/index/introducing-gpt-oss-safeguard/  
ROOST.tools press release: https://roost.tools/blog/a-new-milestone-for-open-source-safety-infrastructure-and-transparency/

Your content goes here. Please use [markdown](https://docs.github.com/fr/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github) for formating.
# title 1

## title 2

I love **EU**
